{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# libraries\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PSD, FFT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music_time_series as mts\n",
    "\n",
    "data = mts.musdat('NACHTMUSIK.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = data[14500000:14525000] / max(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the file\n",
    "name = 'best_777_T_Bz=5A_Bx=0.08A_exc810_det852.3_5mW_5K_sigma+_PEM_(30min).txt'\n",
    "\n",
    "# load data\n",
    "exp_data = np.loadtxt(name, skiprows=1, delimiter='\\t')\n",
    "#t = exp_data[:,0]\n",
    "#rho = exp_data[:,3]\n",
    "\n",
    "t = np.arange(0, len(rho), 1)\n",
    "\n",
    "# fft\n",
    "sp = np.fft.fft(rho)\n",
    "freq_fft = np.fft.fftfreq(t.shape[-1])\n",
    "power_fft = (sp.real)**2\n",
    "power_fft[0] = 0\n",
    "freq, power = signal.periodogram(rho, scaling='spectrum')\n",
    "#freq, power = signal.welch(rho, scaling='spectrum')\n",
    "\n",
    "# signal\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Signal')\n",
    "plt.ylabel(\"$p_c$(%)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.plot(t, rho, 'g', lw=2)\n",
    "plt.xlim([0, max(t)])\n",
    "plt.savefig('/Users/paulsokolov/Downloads/NACHTMUSIK/signal.jpg')\n",
    "plt.show()\n",
    "\n",
    "# power spectrum\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Power spectral density')\n",
    "plt.semilogy(freq, power, 'k')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0, 0.5])\n",
    "axes.set_ylim([10**-15, max(power)+max(power)*10])\n",
    "plt.savefig('/Users/paulsokolov/Downloads/NACHTMUSIK/psd.jpg')\n",
    "plt.show()\n",
    "\n",
    "# fft\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'FFT')\n",
    "plt.plot(freq, power, 'r')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,0.5])\n",
    "axes.set_ylim([0,max(power)+max(power)*0.1])\n",
    "axes.fill_between(freq, 0, power, facecolor='red')\n",
    "plt.xlabel(r'Frequency (Hz)')\n",
    "plt.ylabel(r'(arb. un.)')\n",
    "plt.savefig('/Users/paulsokolov/Downloads/NACHTMUSIK/fff.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autocorrelation**\n",
    "\n",
    "The autocorrelation of a signal describes the similarity of a signal against a time-shifted version of itself. For a signal $x$, the autocorrelation $r$ is:\n",
    "$$r(k)=\\sum_nx(n)x(n−k)$$\n",
    " \n",
    "In this equation, $k$ is often called the lag parameter.  $r(k)$ is maximized at  $k=0$ and is symmetric about $k$.\n",
    "The autocorrelation is useful for finding repeated patterns in a signal. For example, at short lags, the autocorrelation can tell us something about the signal's fundamental frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(rho)\n",
    "fvi = np.fft.fft(rho, n=2*N)\n",
    "acf = np.real(np.fft.ifft(fvi * np.conjugate(fvi))[:N])\n",
    "acf = acf / (N - np.arange(N))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Autocorrelation')\n",
    "plt.xlabel(r'Time delay, $t$')\n",
    "plt.ylabel(r'$r(t)$')\n",
    "plt.plot(t[0:len(acf) - int(len(acf) / 2)], acf[0:len(acf) - int(len(acf) / 2)], 'r')\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.savefig('/Users/paulsokolov/Downloads/NACHTMUSIK/autocorrelation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time delayed mutual information was suggested by Fraser and Swinney as a tool to determine a reasonable delay: Unlike the autocorrelation function, the mutual information takes into account also nonlinear correlations. One has to compute\n",
    "$$\n",
    "S = - \\sum_{ij} p_{ij}(\\tau) ln \\frac{p_{ij}(\\tau)}{p_ip_j}, \n",
    "$$\n",
    "where for some partition on the real numbers $p_{ij}$ is the probability to find a time series value in the $i$-th interval, and $p_{ij}(\\tau)$ is the joint probability that an observation falls into the $i$-th interval and the observation time $\\tau$ later falls into the $j$-th. In theory this expression has no systematic dependence on the size of the partition elements and can be quite easily computed. There exist good arguments that if the time delayed mutual information exhibits a marked minimum at a certain value of $\\tau$, then this is a good candidate for a reasonable time delay. However, these arguments have to be modified when the embedding dimension exceeds two. Moreover, as will become transparent in the following sections, not all applications work optimally with the same delay. Since we are not really interested in absolute values of the mutual information here but rather in the first minimum, the minimal implementation given here seems to be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mut(x, y, bins=32):\n",
    "    \"\"\"\n",
    "    Calculates mutual information, I = S(x) + S(y) - S(x,y), between x and y.\n",
    "    Here, S(x) is the Shannon entropy.\n",
    "    _____\n",
    "    Input (x : array, y : array, bins : int {bins for histogram})\n",
    "    Output (i : float {mutual information})\n",
    "    \"\"\"\n",
    "    p_x = np.histogram(x, bins)[0]\n",
    "    p_y = np.histogram(y, bins)[0]\n",
    "    p_xy = np.histogram2d(x, y, bins)[0].flatten()\n",
    "\n",
    "    # convert frequencies into  the probabilities\n",
    "    p_x = p_x[p_x > 0] / np.sum(p_x)\n",
    "    p_y = p_y[p_y > 0] / np.sum(p_y)\n",
    "    p_xy = p_xy[p_xy > 0] / np.sum(p_xy)\n",
    "\n",
    "    # calculate the Shannon entropies\n",
    "    Sh_x = np.sum(p_x * np.log2(p_x))\n",
    "    Sh_y = np.sum(p_y * np.log2(p_y))\n",
    "    Sh_xy = np.sum(p_xy * np.log2(p_xy))\n",
    "\n",
    "    return Sh_xy - Sh_x - Sh_y\n",
    "\n",
    "\n",
    "def tdmut(x, tau, bins=32):\n",
    "    \"\"\"\n",
    "    Calculate the time-delayed mutual information.\n",
    "    _____\n",
    "    Input (x : array {1d time series}, tau : int {maximal time delay}, bins : int {bins for histogram})\n",
    "    Output (arr : float {array with the time-delayed mutual information})\n",
    "    \"\"\"\n",
    "    tau_max = min(len(x), tau)\n",
    "    arr = np.empty(tau_max)\n",
    "    arr[0] = mut(x, x, bins)\n",
    "\n",
    "    for i in range(1, tau_max):\n",
    "        arr[i] = mut(x[:-i], x[i:], bins)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "calc_range = 50\n",
    "mutual_inf = tdmut(rho, tau=calc_range, bins=2)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(mutual_inf, 'ro-')\n",
    "plt.xticks(np.arange(0, calc_range, 1.0))\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.title(r'Delayed mutual information')\n",
    "plt.xlabel(r'Delay, $\\tau$')\n",
    "plt.ylabel(r'$I(\\tau)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase space reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "# delay\n",
    "r_delay = 10\n",
    "\n",
    "# filter\n",
    "#sos = signal.butter(2, 0.5, 'lp', output='sos')\n",
    "#rho1 = signal.sosfilt(sos, rho)\n",
    "b, a = signal.butter(5, 0.2)\n",
    "rho1 = signal.filtfilt(b, a, rho)\n",
    "\n",
    "X = rho[:-r_delay*2]\n",
    "Y = rho[r_delay:-r_delay]\n",
    "Z = rho[2*r_delay:]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "\n",
    "# rotation step\n",
    "step=2\n",
    "\n",
    "def turn(xx,yy):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    #ax.set_xlabel(r'$x(t)$')\n",
    "    #ax.set_ylabel(r'$x(t + \\tau)$')\n",
    "    #ax.set_zlabel(r'$x(t + 2\\tau)$');\n",
    "    ax.view_init(xx, yy)\n",
    "    ax.plot(X[::step], Y[::step], Z[::step],'-r', lw=0.5)\n",
    "    ax.scatter(X, Y, Z, c=np.sqrt(X**2+Y**2), cmap='magma', s=10)\n",
    "    ax.set_facecolor('k')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('/Users/paulsokolov/Downloads/NACHTMUSIK/3d.png')\n",
    "    plt.show()\n",
    "    \n",
    "interact(turn, xx=FloatSlider(value=55, min=0, max=360, step=1),\n",
    "         yy=FloatSlider(value=75, min=0, max=90, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation sum and correlation dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to to quantify the self-similarity of a geometrical object by a dimension, i.e. correlated dimension.\n",
    "The correlation sum for a collection of pints $x_n$ in some vector space to be the fraction of all possible pairs of points which are closer than a given distance $\\epsilon$ in a particular norm. The basic formula is \n",
    "$$\n",
    "C(\\epsilon) = \\frac{2}{N(N-1)} \\sum_{i=1}^N \\sum_{j=i+1}^N \\Theta (\\epsilon -||x_i - x_j||),\n",
    "$$\n",
    "where $\\Theta$ is Heaviside step function. The sum just counts the pairs $(x_i,x_j)$ whose distance is smaller than $\\epsilon$. In the limit of an infinite amount of data ($N \\rightarrow \\infty$) and for smaller $\\epsilon$, $C$ scales like a power law, $C(\\epsilon) \\sim \\epsilon^D$, and the correlated dimension defined as\n",
    "$$\n",
    "d(N,\\epsilon) = \\frac{\\partial \\mathrm{ln} C (\\epsilon,N)}{\\partial  \\mathrm{ln} \\epsilon}, \\\\\n",
    "        D = \\lim\\limits_{\\epsilon \\to 0} \\lim\\limits_{N \\to \\infty} d(N, \\epsilon).\n",
    "$$\n",
    "Instead of directly inspecting the correlation sum plots, it is often customary to plot the local slope,\n",
    "$$\n",
    "        D_2^{(d)} = \\frac{d log C_2^{(d)} (r)}{d log r},\n",
    "$$\n",
    "as a function of $r$ in order to identify this scaling region. Scaling at extreme length scales is often impaired by “edge effects,” i.e., at very small length scales, noise and poor statistics create fluctuations in $D_2^{(d)}(r)$, and at very large length scales, finiteness of attractor size causes it to drop to zero. A scaling region with a fairly constant $D_2(d)$ is generally found at intermediate length scales sandwiched between these two extremes. If the time series comes out of a low-dimensional chaotic process, then one expects $D_2(d)$ to converge to the $D_2$ of the underlying attractor within a sufficiently small $d$ (i.e., at the minimum embedding dimension). On the other hand, for stochastic processes, the delay vectors fill up the phase space, causing $D_2(d)$ to diverge with $d$. For the special case of uncorrelated noise, $D_2(d)$ would be equal to $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> *Correlation sum*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def delayed_vectros(x, dim, tau):\n",
    "    \"\"\"\n",
    "    Calculates the time-delayed vectors from a time series.\n",
    "    _____\n",
    "    Input (x : array {time series}, dim : int {embedding dimension}, tau : int {time delay})\n",
    "    Output (n-dim array {time-delayed array})\n",
    "    \"\"\"\n",
    "    m = len(x) - tau * (dim - 1)\n",
    "        \n",
    "    return np.asarray([x[i:i + tau * (dim - 1) + 1:tau] for i in range(m)])\n",
    "\n",
    "\n",
    "def c2(x, start=0.001, end=1, step=100, window=1):\n",
    "    \"\"\"\n",
    "    Computation the correlation sum of the time series using Grassberger & Procaccia algorithm(1983).\n",
    "    _____\n",
    "    Input (x : n-dim array {points in the phase space}, start : int {starting point of the progression}, \n",
    "           stop : int {ending point of the progression}, step : int {number of points between start and end},\n",
    "           window : int {Theiler window})\n",
    "    Output (r : array {distances for which correlation sums is calculated}, c2 : array {correlation sum C(r)})\n",
    "    \"\"\"\n",
    "    ratio = (end / start) ** (1.0 / (step - 1))\n",
    "    r = start * ratio ** np.arange(step)\n",
    "    r = np.asarray(r)\n",
    "    r = np.sort(r[r > 0])\n",
    "    \n",
    "    bins = np.insert(r, 0, -1)\n",
    "    c = np.zeros(len(r))\n",
    "    \n",
    "    for i in range(len(x) - window - 1):\n",
    "        dists = distance.cdist([x[i]], x[i + window + 1:], metric='chebyshev')[0]\n",
    "        c += np.histogram(dists, bins=bins)[0]\n",
    "        \n",
    "    pairs = 0.5 * (len(x) - window - 1) * (len(x) - window)\n",
    "    c = np.cumsum(c) / pairs\n",
    "    \n",
    "    return r[c > 0], c[c > 0]\n",
    "\n",
    "# number of calculated embedding dimension:\n",
    "m0 = 10\n",
    "dim = np.arange(1, m0 + 1)\n",
    "\n",
    "vectors = [delayed_vectros(rho1, d, r_delay) for d in dim]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Correlation sum')\n",
    "plt.xlabel(r'Distance, $r$')\n",
    "plt.ylabel(r'C(r)')\n",
    "\n",
    "for v in vectors:\n",
    "    x, y = c2(v)\n",
    "    plt.semilogx(x, y, 'r')\n",
    "    \n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> *Correlation dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2(r, c2, window=10):\n",
    "    \"\"\"\n",
    "    Computation the correlation dimension of the time series (C(r) ~ r^D2).\n",
    "    _____\n",
    "    Input (x : array {distances for which correlation sums is been calculated}, c2 : array {correlation sums}, \n",
    "           window : int {Theiler window})\n",
    "    Output (d2 : array {average D2 at each distance})\n",
    "    \"\"\"\n",
    "    hwin = 3\n",
    "    N = len(r) - 2 * hwin\n",
    "    d2 = np.empty(N)\n",
    "    x, y = np.log(r), np.log(c2)\n",
    "\n",
    "    for i in range(N):\n",
    "        p, q = x[i:i + 2 * hwin + 1], y[i:i + 2 * hwin + 1]\n",
    "        A = np.vstack([p, np.ones(2 * hwin + 1)]).T\n",
    "        d2[i] = np.linalg.lstsq(A, q)[0][0]\n",
    "\n",
    "    return d2\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Correlation dimension')\n",
    "plt.xlabel(r'Distance, $r$')\n",
    "plt.ylabel(r'$D_2$')\n",
    "\n",
    "for v in vectors:\n",
    "    x, y = c2(v)\n",
    "    plt.semilogx(x[3:-3], d2(x, y), 'r')\n",
    "    \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0, 10])\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.savefig('/Users/paulsokolov/Downloads/NACHTMUSIK/d2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lyapunov exponents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lyapunov exponents evaluate the sensitive dependence to initial conditions considering the exponential divergence of nearby orbits. Therefore, it is necessary to evaluate how trajectories with nearby initial conditions diverge. The dynamics of the system transforms the $D$-sphere of states in a $D$-ellipsoid and, mathematically, the Lyapunov exponents considers $d(t) = d_0b^{\\lambda t}$, where $b$ is a reference basis. The signs of the Lyapunov exponents provide a qualitative picture of the system’s dynamics. The existence of positive Lyapunov exponents defines directions of local instabilities in the system dynamics. The determination of Lyapunov exponents of dynamical system with an explicitly mathematical model, which can be linearized, is well established from the algorithm proposed by Wolf et al. On the other hand, the determination of these exponents from time series is quite more complex. \n",
    "\n",
    "The average of effective Lyapunov exponent along the trajectory is the true Lyapunov exponent and the maximum value is given by\n",
    "$$\n",
    "\\lambda(t)=\\lim_{x \\to \\infty} \\frac{1}{\\delta} \\mathrm{ln} \\frac{|u(t+\\delta)-u_\\epsilon (t+\\delta)|}{\\epsilon}\n",
    "$$\n",
    "where $|u(0)−u_\\epsilon(0)| = \\epsilon$ and $u(t)−u_\\epsilon(t) = \\epsilon v_u(t)$, with $v_u(t)$ representing the eigenvectors associated with the maximum Lyapunov exponent, $\\lambda_{max}$; $\\delta$ is a relative time referring to the\n",
    "time index of the point where the distance begin to be greater than $\\epsilon$, $\\delta(0)$.\n",
    "\n",
    "If $\\lambda$ is positive, this means an exponential divergence of nearby trajectories, i.e. chaos.\n",
    "In dissipative systems one can also find a negative maximal Lyapunov exponent which reflects the existence of a stable fixed point. Two trajectories which approach th fixed point also approach each other exponentially fast. If the motion down onto a limit cycle, two trajectories can only separate or approach each other slower than exponentially. In this case the maximal Lyapunov exponent is zero and the motion is called marginally stable.\n",
    "\n",
    " The Lyapunov exponent describes the rate of separation of two infinitesimally close trajectories of a dynamical system in phase space. In a chaotic system, these trajectories diverge exponentially following the equation:\n",
    "$|X(t, X_0) - X(t, X_0 + \\epsilon)| = |\\epsilon| e^{\\lambda t}$.\n",
    "\n",
    "In this equation $X(t, X_0)$ is the trajectory of the system $X$ starting at the point $X_0$ in phase space at time $t$. $\\epsilon$ is the (infinitesimal) difference vector and $\\lambda$ is called the Lyapunov exponent. If the system has more than one free variable, the phase space is multidimensional and each dimension has its own Lyapunov exponent. The existence of at least one positive Lyapunov exponent is generally seen as a strong indicator for chaos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lyapunov(data, emb_dim=3, matrix_dim=3, tau=r_delay):\n",
    "    \"\"\"\n",
    "    Estimates the Lyapunov exponents for the given data using the algorithm of Eckmann et al.\n",
    "    _____\n",
    "    Input (x : array {time series}, emb_dim : int {embedding dimension}, \n",
    "           matrix_dim : int {matrix dimension}, tau : float {step size of the data in seconds})\n",
    "    Output (lexp : array {array of matrix_dim Lyapunov exponents})\n",
    "    \"\"\"\n",
    "    data = np.asarray(data)\n",
    "    n = len(data)\n",
    "    \n",
    "    if (emb_dim - 1) % (matrix_dim - 1) != 0:\n",
    "        raise ValueError(\"(emb_dim - 1) must be divisible by the (matrix_dim - 1)!\")\n",
    "    \n",
    "    m = (emb_dim - 1) // (matrix_dim - 1)\n",
    "    min_nb = min(2 * matrix_dim, matrix_dim + 4)\n",
    "    vec_delay = delayed_vectros(x=data, dim=emb_dim, tau=r_delay)\n",
    "    mat_Q_id = np.identity(matrix_dim)\n",
    "    lexp = np.zeros(matrix_dim, dtype=\"float32\")\n",
    "    lexp_counts = np.zeros(lexp.shape)\n",
    "    \n",
    "    for i in range(len(vec_delay)):\n",
    "        # neighbors for each vector\n",
    "        diffs = np.max(np.abs(vec_delay - vec_delay[i]), axis=1)\n",
    "        # all neighbors that are too close in time to the vector itself\n",
    "        diffs[max(0, i):min(len(diffs), i + 1)] = np.inf\n",
    "        indices = np.argsort(diffs)\n",
    "        idx = indices[min_nb - 1]  # index of the min_nb-nearest neighbor\n",
    "        r = diffs[idx]  # corresponding distance\n",
    "        # there may be more than min_nb vectors having a distance of equal to r\n",
    "        indices = np.where(diffs <= r)[0]\n",
    "        # emb_dim = (d_M - 1) * m + 1\n",
    "        mat_X = np.array([data[j:j + emb_dim:m] for j in indices])\n",
    "        mat_X -= data[i:i + emb_dim:m]\n",
    "        # vector for linear least squares\n",
    "        vector = data[indices + matrix_dim * m] - data[i + matrix_dim * m]\n",
    "        a, _, _, _ = np.linalg.lstsq(mat_X, vector, rcond=-1)\n",
    "        # matrix T = \n",
    "        # 0  1  0  ... 0\n",
    "        # 0  0  1  ... 0\n",
    "        # ...\n",
    "        # 0  0  0  ... 1\n",
    "        # a1 a2 a3 ... a_(d_M)\n",
    "        mat_T = np.zeros((matrix_dim, matrix_dim))\n",
    "        mat_T[:-1, 1:] = np.identity(matrix_dim - 1)\n",
    "        mat_T[-1] = a\n",
    "        # QR-decomposition of T * mat_Q_id\n",
    "        mat_Q, mat_R = np.linalg.qr(np.dot(mat_T, mat_Q_id))\n",
    "        # force diagonal of R to be positive (if QR = A then also QLL'R = A with L' = L^-1)\n",
    "        sign_diag = np.sign(np.diag(mat_R))\n",
    "        sign_diag[np.where(sign_diag == 0)] = 1\n",
    "        sign_diag = np.diag(sign_diag)\n",
    "        mat_Q = np.dot(mat_Q, sign_diag)\n",
    "        mat_R = np.dot(sign_diag, mat_R)\n",
    "        mat_Q_id = mat_Q\n",
    "        # sum for Lyapunov exponents\n",
    "        diag_R = np.diag(mat_R)\n",
    "        # drop zeros in mat_R\n",
    "        idx = np.where(diag_R > 0)\n",
    "        lexp_i = np.zeros(diag_R.shape, dtype=\"float32\")\n",
    "        lexp_i[idx] = np.log(diag_R[idx])\n",
    "        lexp_i[np.where(diag_R == 0)] = np.inf\n",
    "        lexp[idx] += lexp_i[idx]\n",
    "        lexp_counts[idx] += 1\n",
    "\n",
    "    # normalize exponents over number of individual mat_Rs\n",
    "    idx = np.where(lexp_counts > 0)\n",
    "    lexp[idx] /= lexp_counts[idx]\n",
    "    lexp[np.where(lexp_counts == 0)] = np.inf\n",
    "    # normalize with respect to tau\n",
    "    lexp /= tau\n",
    "    # normalize with respect to m\n",
    "    lexp /= m\n",
    "    return lexp\n",
    "\n",
    "l_exp = list(lyapunov(rho, emb_dim=3, matrix_dim=3, tau=r_delay))\n",
    "print('Lyapunov exponents:' + str(l_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kaplan–Yorke conjecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In applied mathematics, the Kaplan–Yorke conjecture concerns the dimension of an attractor, using Lyapunov exponents. By arranging the Lyapunov exponents in order from largest to smallest $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\lambda_n$, let j be the index for which\n",
    "$$\n",
    "\\sum^{j}_{i=1} \\lambda_i \\geq 0\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sum^{j+1}_{i=1} \\lambda_i < 0.\n",
    "$$\n",
    "Then the conjecture is that the dimension of the attractor is\n",
    "$$\n",
    "D = j + \\frac{\\sum^{j}_{i=1} \\lambda_i}{|\\lambda_{j+1}|}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arranging the Lyapunov exponents in order from largest to smallest \n",
    "l_exp.sort(reverse=True)\n",
    "\n",
    "k = 0\n",
    "\n",
    "while True:\n",
    "    if l_exp[k]>=0 and l_exp[k+1] < 0:\n",
    "        # dimension\n",
    "        D = k + 1 + sum(l_exp[0:k+1]) / abs(l_exp[k+1])\n",
    "        print(f'The Kaplan–Yorke dimension of the attractor is {round(D, 3)}.')\n",
    "        break\n",
    "    \n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
