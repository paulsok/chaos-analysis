{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PSD, FFT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of the file\n",
    "name='best_777_T_Bz=5A_Bx=0.08A_exc810_det852.3_5mW_5K_sigma+_PEM_(30min).txt'\n",
    "\n",
    "exp_data = np.loadtxt(name, skiprows=1, delimiter='\\t')\n",
    "t=exp_data[:,0]\n",
    "rho=exp_data[:,3]\n",
    "\n",
    "#FFT\n",
    "sp = np.fft.fft(rho)\n",
    "freq_fft = np.fft.fftfreq(t.shape[-1])\n",
    "power_fft=(sp.real)**2\n",
    "power_fft[0]=0\n",
    "\n",
    "freq, power = signal.periodogram(rho, scaling='spectrum')\n",
    "#freq, power = signal.welch(rho, scaling='spectrum')\n",
    "\n",
    "font = {'size': 20}\n",
    "\n",
    "#Main signal\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Signal')\n",
    "plt.ylabel(\"$p_c$(%)\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.plot(t, rho, 'g', lw=2)\n",
    "plt.xlim([0,max(t)])\n",
    "plt.rc('font', **font)\n",
    "\n",
    "#Power spectrum\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Power spectral density')\n",
    "plt.semilogy(freq, power, 'k')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('PSD [V**2/Hz]')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,0.5])\n",
    "axes.set_ylim([10**-15,max(power)+max(power)*10])\n",
    "\n",
    "#FFT\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'FFT')\n",
    "plt.plot(freq, power, 'r')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0,0.5])\n",
    "axes.set_ylim([0,max(power)+max(power)*0.1])\n",
    "axes.fill_between(freq, 0, power, facecolor='red')\n",
    "plt.xlabel(r'Frequency (Hz)')\n",
    "plt.ylabel(r'(arb. un.)')\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autocorrelation**\n",
    "\n",
    "The autocorrelation of a signal describes the similarity of a signal against a time-shifted version of itself. For a signal $x$, the autocorrelation $r$ is:\n",
    "$$r(k)=∑_nx(n)x(n−k)$$\n",
    " \n",
    "In this equation, $k$ is often called the lag parameter.  $r(k)$ is maximized at  $k=0$ and is symmetric about $k$.\n",
    "The autocorrelation is useful for finding repeated patterns in a signal. For example, at short lags, the autocorrelation can tell us something about the signal's fundamental frequency. For longer lags, the autocorrelation may tell us something about the tempo of a musical signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(rho)\n",
    "fvi = np.fft.fft(rho, n=2*N)\n",
    "acf = np.real(np.fft.ifft(fvi * np.conjugate(fvi))[:N])\n",
    "acf = acf/(N - np.arange(N))\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Autocorrelation')\n",
    "plt.xlabel(r'Time delay, $t$')\n",
    "plt.ylabel(r'$r(t)$')\n",
    "plt.plot(t[0:len(acf)-int(len(acf)/2)], acf[0:len(acf)-int(len(acf)/2)], 'r')\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.rc('font', **font)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time delayed mutual information was suggested by Fraser and Swinney as a tool to determine a reasonable delay: Unlike the autocorrelation function, the mutual information takes into account also nonlinear correlations. One has to compute\n",
    "$$\n",
    "S = - \\sum_{ij} p_{ij}(\\tau) ln \\frac{p_{ij}(\\tau)}{p_ip_j}, \n",
    "$$\n",
    "where for some partition on the real numbers $p_{ij}$ is the probability to find a time series value in the $i$-th interval, and $p_{ij}(\\tau)$ is the joint probability that an observation falls into the $i$-th interval and the observation time $\\tau$ later falls into the $j$-th. In theory this expression has no systematic dependence on the size of the partition elements and can be quite easily computed. There exist good arguments that if the time delayed mutual information exhibits a marked minimum at a certain value of $\\tau$, then this is a good candidate for a reasonable time delay. However, these arguments have to be modified when the embedding dimension exceeds two. Moreover, as will become transparent in the following sections, not all applications work optimally with the same delay. Since we are not really interested in absolute values of the mutual information here but rather in the first minimum, the minimal implementation given here seems to be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mut(x, y, bins=32):\n",
    "    \"\"\"\n",
    "    Calculates mutual information, I = S(x) + S(y) - S(x,y), between x and y.\n",
    "    S(x) is the Shannon entropy.\n",
    "    _____\n",
    "    Input (x : array, y : array, bins : int {bins for histogram})\n",
    "    Output (i : float {mutual information})\n",
    "    \"\"\"\n",
    "    p_x = np.histogram(x, bins)[0]\n",
    "    p_y = np.histogram(y, bins)[0]\n",
    "    p_xy = np.histogram2d(x, y, bins)[0].flatten()\n",
    "\n",
    "    #Recalculate frequencies into  the probabilities\n",
    "    p_x = p_x[p_x > 0] / np.sum(p_x)\n",
    "    p_y = p_y[p_y > 0] / np.sum(p_y)\n",
    "    p_xy = p_xy[p_xy > 0] / np.sum(p_xy)\n",
    "\n",
    "    #Calculate the Shannon entropies\n",
    "    Sh_x = np.sum(p_x * np.log2(p_x))\n",
    "    Sh_y = np.sum(p_y * np.log2(p_y))\n",
    "    Sh_xy = np.sum(p_xy * np.log2(p_xy))\n",
    "\n",
    "    return Sh_xy - Sh_x - Sh_y\n",
    "\n",
    "\n",
    "def tdmut(x, tau, bins=32):\n",
    "    \"\"\"\n",
    "    Calculate the time-delayed mutual information.\n",
    "    _____\n",
    "    Input (x : array {1d time series}, tau : int {maximal time delay}, bins : int {bins for histogram})\n",
    "    Output (arr : float {array with the time-delayed mutual information})\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    tau_max = min(n, tau)\n",
    "\n",
    "    arr = np.empty(tau_max)\n",
    "    \n",
    "    arr[0] = mut(x, x, bins)\n",
    "\n",
    "    for i in range(1, tau_max):\n",
    "        arr[i] = mut(x[:-i], x[i:], bins)\n",
    "\n",
    "    return arr\n",
    "\n",
    "tau = 20\n",
    "\n",
    "mutual_inf = tdmut(rho, tau=tau, bins=2)\n",
    "\n",
    "#Plotting the delayed mutual information\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(mutual_inf, label=(r'Bins = 2'), c='r')\n",
    "plt.xticks(np.arange(0, tau, 2.0))\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.title(r'Delayed mutual information')\n",
    "plt.xlabel(r'Delay, $\\tau$')\n",
    "plt.ylabel(r'$I(\\tau)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test for determinism - method of false nearest neighbors introduced by Kennel et al. (1992). It was originally developed as an alternative to the approach of observing the saturation in invariants to determine the minimum embedding dimension required to reconstruct an attractor. If the attractor has not unfolded properly, it will contain a large number of “false nearest neighbors,” i.e., points that are close together solely due to trajectory crossings caused by the projection of the attractor onto a phase space of smaller dimension. Such neighbors no longer remain close once we increase the embedding dimension, telling us that the attractor cannot be fully unfolded at the chosen embedding dimension. Operationally, we consider a point $y^{(d)}_i$ and its nearest neighbor $y^{(d)}_{n (i,d)}$ in the $d$-dimensional reconstructed phase space as false if\n",
    "$$\n",
    "\\frac{|x_{i+d\\tau} - x_{n(i,d)+d\\tau}|} {||y^{(d)}_i - y^{(d)}_{n(i,d)} ||}>A,\n",
    "$$\n",
    "where $||\\cdot||$ is the Euclidean norm and $A$ is a suitable threshold. We test this condition for all points in the phase space and compute the total fraction of false near neighbors for each $d$. If this fraction becomes zero at a finite, and preferably small, $d$ (the minimum embedding dimension $d_{min}$), then we can conclude that the attractor has properly unfolded and that the time series comes out of a deterministic process. In other words, the system generating the time series is low-dimensional enough to allow its attractor to be embedded in a phase space of $d_{min}$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "#Delay from mutual information\n",
    "r_delay=4\n",
    "\n",
    "#Number of calculated embedding dimension:\n",
    "m0=10\n",
    "dim = np.arange(1, m0 + 1)\n",
    "\n",
    "\n",
    "def delayed_vectros(x, dim, tau):\n",
    "    \"\"\"\n",
    "    Calculates the time-delayed vectors from a time series.\n",
    "    _____\n",
    "    Input (x : array {time series}, dim : int {embedding dimension}, tau : int {time delay})\n",
    "    Output (n-dim array {time-delayed array})\n",
    "    \"\"\"\n",
    "    m = len(x) - tau * (dim - 1)\n",
    "        \n",
    "    return np.asarray([x[i:i + tau * (dim - 1) + 1:tau] for i in range(m)])\n",
    "\n",
    "\n",
    "def n_neighbors(x, window=0):\n",
    "    \"\"\"\n",
    "    Finds the nearest neighbors for all points in the array with KDTree search (SciPy).\n",
    "    _____\n",
    "    Input (y : n-dim array {time-delayed vectors}, window : int {Theiler window})\n",
    "    Output (indices : array {indices of near neighbors}, distances : array {neighbor distances})\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    max_n = 2 * (window + 1) + 1\n",
    "    indices = np.empty(n, dtype=int)\n",
    "    distances = np.empty(n)\n",
    "    \n",
    "    tree = KDTree(x)\n",
    "    \n",
    "    for i, j in enumerate(x):\n",
    "        \n",
    "        for k in range(2, max_n + 2):\n",
    "            dist, index = tree.query(j, k=k, p=np.inf)\n",
    "            valid = (np.abs(index - i) > window) & (dist > 0)\n",
    "\n",
    "            if np.count_nonzero(valid):\n",
    "                distances[i] = dist[valid][0]\n",
    "                indices[i] = index[valid][0]\n",
    "                break\n",
    "                \n",
    "    return np.squeeze(indices), np.squeeze(distances)\n",
    "\n",
    "\n",
    "def fnn(d, x, tau, threshold=10):\n",
    "    y1 = delayed_vectros(x[:-tau], d, tau)\n",
    "    y2 = delayed_vectros(x, d + 1, tau)\n",
    "\n",
    "    index, dist = n_neighbors(y1)\n",
    "\n",
    "    # Kennel's test\n",
    "    f = np.abs(y2[:, -1] - y2[index, -1]) / dist > threshold\n",
    "\n",
    "    return np.mean(f)\n",
    "    \n",
    "\n",
    "f_nn = [100 * fnn(i, rho, tau=r_delay) for i in dim]\n",
    "\n",
    "#Plotting the FNN\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'False nearest neighbors')\n",
    "plt.xlabel(r'Embedding dimension, $d$')\n",
    "plt.ylabel(r'FNN (%)')\n",
    "plt.plot(dim, f_nn, 'ro-')\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase space reconstruction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "# Minimum FNN\n",
    "m_fnn=6\n",
    "\n",
    "# Filter\n",
    "#sos = signal.butter(2, 0.5, 'lp', output='sos')\n",
    "#rho1 = signal.sosfilt(sos, rho)\n",
    "b, a = signal.butter(5, 0.25)\n",
    "rho1 = signal.filtfilt(b, a, rho)\n",
    "\n",
    "X = rho1[:-r_delay*2]\n",
    "Y = rho1[r_delay:-r_delay]\n",
    "Z = rho1[2*r_delay:]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "\n",
    "step=1\n",
    "\n",
    "def turn(xx,yy):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    #ax.set_xlabel(r'$x(t)$')\n",
    "    #ax.set_ylabel(r'$x(t + \\tau)$')\n",
    "    #ax.set_zlabel(r'$x(t + 2\\tau)$');\n",
    "\n",
    "    ax.view_init(xx, yy)\n",
    "    ax.plot(X[::step], Y[::step], Z[::step],'-r', lw=0.5)\n",
    "    ax.scatter(X, Y, Z, c=np.sqrt(X**2+Y**2), cmap='magma', s=10)\n",
    "    ax.set_facecolor('k')\n",
    "    plt.axis('off')\n",
    "    #plt.savefig(name+'3D1'+'.pdf')\n",
    "    \n",
    "interact(turn,xx=FloatSlider(value=55, min=0, max=360, step=1),\n",
    "         yy=FloatSlider(value=75, min=0, max=90, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation sum and correlation dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to to quantify the self-similarity of a geometrical object by a dimension, i.e. correlated dimension.\n",
    "The correlation sum for a collection of pints $x_n$ in some vector space to be the fraction of all possible pairs of points which are closer than a given distance $\\epsilon$ in a particular norm. The basic formula is \n",
    "$$\n",
    "C(\\epsilon) = \\frac{2}{N(N-1)} \\sum_{i=1}^N \\sum_{j=i+1}^N \\Theta (\\epsilon -||x_i - x_j||),\n",
    "$$\n",
    "where $\\Theta$ is Heaviside step function. The sum just counts the pairs $(x_i,x_j)$ whose distance is smaller than $\\epsilon$. In the limit of an infinite amount of data ($N \\rightarrow \\infty$) and for smaller $\\epsilon$, $C$ scales like a power law, $C(\\epsilon) \\sim \\epsilon^D$, and the correlated dimension defined as\n",
    "$$\n",
    "d(N,\\epsilon) = \\frac{\\partial \\mathrm{ln} C (\\epsilon,N)}{\\partial  \\mathrm{ln} \\epsilon}, \\\\\n",
    "        D = \\lim\\limits_{\\epsilon \\to 0} \\lim\\limits_{N \\to \\infty} d(N, \\epsilon).\n",
    "$$\n",
    "Instead of directly inspecting the correlation sum plots, it is often customary to plot the local slope,\n",
    "$$\n",
    "        D_2^{(d)} = \\frac{d log C_2^{(d)} (r)}{d log r},\n",
    "$$\n",
    "as a function of $r$ in order to identify this scaling region. Scaling at extreme length scales is often impaired by “edge effects,” i.e., at very small length scales, noise and poor statistics create fluctuations in $D_2^{(d)}(r)$, and at very large length scales, finiteness of attractor size causes it to drop to zero. A scaling region with a fairly constant $D_2(d)$ is generally found at intermediate length scales sandwiched between these two extremes. If the time series comes out of a low-dimensional chaotic process, then one expects $D_2(d)$ to converge to the $D_2$ of the underlying attractor within a sufficiently small $d$ (i.e., at the minimum embedding dimension). On the other hand, for stochastic processes, the delay vectors fill up the phase space, causing $D_2(d)$ to diverge with $d$. For the special case of uncorrelated noise, $D_2(d)$ would be equal to $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Correlation sum*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def c2(x, start=0.1, end=10, step=100, window=10):\n",
    "    \"\"\"\n",
    "    Computation the correlation sum of the time series using Grassberger & Procaccia algorithm(1983).\n",
    "    _____\n",
    "    Input (x : n-dim array {points in the phase space}, start : int {starting point of the progression}, \n",
    "           stop : int {ending point of the progression}, step : int {number of points between start and end},\n",
    "           window : int {Theiler window})\n",
    "    Output (r : array {distances for which correlation sums is calculated}, c2 : array {correlation sum C(r)})\n",
    "    \"\"\"\n",
    "    ratio = (end / start) ** (1.0 / (step - 1))\n",
    "    r = start * ratio ** np.arange(step)\n",
    "    r = np.asarray(r)\n",
    "    r = np.sort(r[r > 0])\n",
    "    \n",
    "    bins = np.insert(r, 0, -1)\n",
    "    c = np.zeros(len(r))\n",
    "    n = len(x)\n",
    "    \n",
    "    for i in range(n - window - 1):\n",
    "        dists = distance.cdist([x[i]], x[i + window + 1:], metric='chebyshev')[0]\n",
    "        c += np.histogram(dists, bins=bins)[0]\n",
    "        \n",
    "    pairs = 0.5 * (n - window - 1) * (n - window)\n",
    "    c = np.cumsum(c) / pairs\n",
    "    \n",
    "    return r[c > 0], c[c > 0]\n",
    "\n",
    "vectors = [delayed_vectros(rho, d, r_delay) for d in dim]\n",
    "\n",
    "#Plotting the correlation sum\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Correlation sum')\n",
    "plt.xlabel(r'Distance, $r$')\n",
    "plt.ylabel(r'C(r)')\n",
    "for v in vectors:\n",
    "    x, y = c2(v)\n",
    "    plt.semilogx(x, y, 'r')\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Correlation dimension*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2(r, c2, window=10):\n",
    "    \"\"\"\n",
    "    Computation the correlation dimension of the time series (C(r) ~ r^D2).\n",
    "    _____\n",
    "    Input (x : array {distances for which correlation sums is been calculated}, c2 : array {correlation sums}, \n",
    "           window : int {Theiler window})\n",
    "    Output (d2 : array {average D2 at each distance})\n",
    "    \"\"\"\n",
    "    hwin = 3\n",
    "    N = len(r) - 2 * hwin\n",
    "    d2 = np.empty(N)\n",
    "    x, y = np.log(r), np.log(c2)\n",
    "\n",
    "    for i in range(N):\n",
    "        p, q = x[i:i + 2 * hwin + 1], y[i:i + 2 * hwin + 1]\n",
    "        A = np.vstack([p, np.ones(2 * hwin + 1)]).T\n",
    "        d2[i] = np.linalg.lstsq(A, q)[0][0]\n",
    "\n",
    "    return d2\n",
    "\n",
    "#Plotting the correlation dimension\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(r'Correlation dimension')\n",
    "plt.xlabel(r'Distance, $r$')\n",
    "plt.ylabel(r'$D_2$')\n",
    "\n",
    "for v in vectors:\n",
    "    x, y = c2(v)\n",
    "    plt.semilogx(x[3:-3], d2(x, y), 'r')\n",
    "    \n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0.1, 10])\n",
    "axes.set_ylim([0, 5])\n",
    "plt.grid(color='black', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
